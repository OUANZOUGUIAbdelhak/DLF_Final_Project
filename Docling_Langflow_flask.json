{
  "id": "a7e4b6a1-d444-487c-bec7-a954e6d42725",
  "data": {
    "nodes": [
      {
        "id": "Prompt-F8PHW",
        "type": "genericNode",
        "position": {
          "x": 1032.6045430601127,
          "y": 1222.2856066066852
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-F8PHW",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "Veuillez lire le document ci-dessous et extraire les informations demandées. Formatez les réponses clairement pour chaque type de verre et essaye de faire ton mieux pour extraire toutes les inforamtion dans la meilleure façone et de ne pas se limiter de demander à l'utilisateur de continuer à remplir les valeurs. Utilisez le format suivant pour chaque type de verre :\n\n---\n\n**Document :**\n{Document}\n\n---\n\n**Informations demandées :**\n\n1. **Type du document :** [Type clair, sans texte additionnel]\n2. **Titre du document :** [Titre clair, sans texte additionnel]\n3. **Référence :** [Référence claire, sans texte additionnel]\n4. **Premier Auteur :** [Nom complet clair, sans texte additionnel]\n5. **Nombre de types de verres :** [Nombre] il faut détecter le nombre total des verres même si je demande seulement 8\n\n**Pour les trois premiers types de verre :**\n\n6. **Verre_type1 :** [Description ou identifiant du type de verre]\n7. **SiO₂(Verre_type1) :** [Valeur numérique]\n8. **B₂O₃(Verre_type1) :** [Valeur numérique]\n9. **Na₂O(Verre_type1) :** [Valeur numérique]\n10. **Al₂O₃(Verre_type1) :** [Valeur numérique]\n11. **Fines(Verre_type1) :** [Valeur numérique]\n12. **Densité(Verre_type1) :** [Valeur numérique]\n13. **Homogénéité(Verre_type1) :** [Commentaire]\n14. **% B(IV)(Verre_type1) :** [Valeur numérique]\n15. **Irradié(Verre_type1) :** [O/N]\n16. **Caractéristiques si irradié(Verre_type1) :** [Commentaire]\n17. **Température(Verre_type1) :** [Valeur numérique]\n18. **Statique/dynamique(Verre_type1) :** [Commentaire]\n19. **Plage granulo si poudre(Verre_type1) :** [Valeur numérique]\n20. **Surface spécifique géométrique si poudre(Verre_type1) :** [Valeur numérique]\n21. **Surface spécifique BET si poudre(Verre_type1) :** [Valeur numérique]\n22. **Qualité polissage si monolithe(Verre_type1) :** [Commentaire]\n23. **Masse verre(Verre_type1) :** [Valeur numérique]\n24. **S(verre)(Verre_type1) :** [Valeur numérique]\n25. **V(solution)(Verre_type1) :** [Valeur numérique]\n26. **Débit solution(Verre_type1) :** [Valeur numérique]\n27. **pH initial (T amb)(Verre_type1) :** [Valeur numérique]\n28. **pH initial (T essai)(Verre_type1) :** [Valeur numérique]\n29. **Compo solution(Verre_type1) :** [Commentaire]\n30. **Durée expérience(Verre_type1) :** [Valeur numérique]\n31. **pH final (T amb)(Verre_type1) :** [Valeur numérique]\n32. **pH final (T essai)(Verre_type1) :** [Valeur numérique]\n33. **Normalisation vitesse (Spm ou SBET)(Verre_type1) :** [Valeur numérique]\n34. **V₀(Si)(Verre_type1) :** [Valeur numérique]\n35. **r²(Si)(Verre_type1) :** [Valeur numérique]\n36. **Ordonnée origine(Verre_type1) :** [Valeur numérique]\n37. **V₀(B)(Verre_type1) :** [Valeur numérique]\n38. **Ordonnée origine(Verre_type1) :** [Valeur numérique]\n39. **V₀(Na)(Verre_type1) :** [Valeur numérique]\n40. **r²(Na)(Verre_type1) :** [Valeur numérique]\n41. **Ordonnée origine(Verre_type1) :** [Valeur numérique]\n42. **V₀(ΔM)(Verre_type1) :** [Valeur numérique]\n43. **Congruence(Verre_type1) :** [Commentaire]\n\n**Répétez les étapes ci-dessus pour Verre_type2, Verre_type3 et Verre_type4 Jusqu'au Verre_type8 en incrémentant les numéros.**\n\n---\n\n**Format attendu :**\n\n1. Type du document : [Type]\n2. Titre du document : [Titre]\n3. Référence : [Référence]\n4. Premier Auteur : [Premier Auteur]\n5. Nombre de types de verres : [Nombre]\n6. Verre_type1 : [Type1]\n7. SiO₂(Verre_type1) : [Valeur]\n8. B₂O₃(Verre_type1) : [Valeur]\n9. Na₂O(Verre_type1) : [Valeur]\n10. Al₂O₃(Verre_type1) : [Valeur]\n11. Fines(Verre_type1) : [Valeur]\n12. Densité(Verre_type1) : [Valeur]\n...\n43. Congruence(Verre_type1) : [Commentaire]\n44. Verre_type2 : [Type2]\n45. SiO₂(Verre_type2) : [Valeur]\n46. B₂O₃(Verre_type2) : [Valeur]\n...\n87. Congruence(Verre_type2) : [Commentaire]\n88. Verre_type3 : [Type3]\n89. SiO₂(Verre_type3) : [Valeur]\n90. B₂O₃(Verre_type3) : [Valeur]\n...\n131. Congruence(Verre_type3) : [Commentaire]\n...\n169. Congruence(Verre_type4) : [Commentaire]\n...\n348. Congruence(Verre_type8) : [Commentaire]\n\n---\nIl faut absolument donner toutes les valeurs des 8 types de verres de 1 à 348,  et  il faut surtout pas ajouter pas de commentaire à la fin.\nSi une des informations est indisponible dans le document, indiquez \"Non disponible\"."
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt1Line",
            "documentation": "",
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "Prompt"
        },
        "selected": false,
        "width": 320,
        "height": 346,
        "positionAbsolute": {
          "x": 1032.6045430601127,
          "y": 1222.2856066066852
        },
        "dragging": false
      },
      {
        "id": "ParseData-ggwft",
        "type": "genericNode",
        "position": {
          "x": 562.4948625060265,
          "y": 1616.1791176371526
        },
        "data": {
          "description": "Convert Data into plain text following a specified template.",
          "display_name": "Parse Data",
          "id": "ParseData-ggwft",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Convert Data into plain text following a specified template.",
            "display_name": "Parse Data",
            "documentation": "",
            "edited": false,
            "field_order": [
              "data",
              "template",
              "sep"
            ],
            "frozen": false,
            "icon": "braces",
            "legacy": false,
            "lf_version": "1.1.1",
            "metadata": {},
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Text",
                "method": "parse_data",
                "name": "text",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n"
              },
              "data": {
                "advanced": false,
                "display_name": "Data",
                "dynamic": false,
                "info": "The data to convert to text.",
                "input_types": [
                  "Data"
                ],
                "list": false,
                "name": "data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "sep": {
                "advanced": true,
                "display_name": "Separator",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "sep",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n"
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              }
            }
          },
          "type": "ParseData"
        },
        "selected": false,
        "width": 320,
        "height": 302,
        "positionAbsolute": {
          "x": 562.4948625060265,
          "y": 1616.1791176371526
        },
        "dragging": false
      },
      {
        "id": "CustomComponent-OsOFR",
        "type": "genericNode",
        "position": {
          "x": 3217.1431542364103,
          "y": 1795.363494913002
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import Output, MessageTextInput\nfrom langflow.schema import Data\nimport requests\n\nclass EnvoyerDonneesVerreTableComponent(Component):\n    display_name = \"Envoyer Données Verre à la Table\"\n    description = \"Envoyer la composition détaillée du verre et les informations de référence du document au serveur Flask.\"\n    icon = \"table\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"texte_extrait\",\n            display_name=\"Texte Extrait\",\n            info=(\n                \"Texte extrait contenant la référence du document et les informations sur la composition du verre.\"\n            ),\n            value=(\n                \"1. Type du document : Article scientifique\\n\"\n                \"2. Titre du document : Can a simple topological-constraints-based model predict the initial dissolution rate of borosilicate and aluminosilicate glasses?\\n\"\n                \"3. Référence : npj Materials Degradation (2020) 4:6 ; https://doi.org/10.1038/s41529-020-0111-4\\n\"\n                \"4. Premier Auteur : Stéphane Gin\\n\"\n                \"5. Nombre de types de verres : 3\\n\"\n                \"6. Verre_type1 : NBS14/18\\n\"\n                \"7. SiO₂(Verre_type1) : 67.8\\n\"\n                \"8. B₂O₃(Verre_type1) : 18.0\\n\"\n                \"9. Na₂O(Verre_type1) : 14.2\\n\"\n                \"10. Al₂O₃(Verre_type1) : 0\\n\"\n                \"11. Fines(Verre_type1) : 100-125 µm\\n\"\n                \"12. Densité(Verre_type1) : 2.451\\n\"\n                \"13. Homogénéité(Verre_type1) : Homogène\\n\"\n                \"14. % B(IV)(Verre_type1) : 66\\n\"\n                \"15. Irradié(Verre_type1) : N\\n\"\n                \"16. Caractéristiques si irradié(Verre_type1) : Non disponible\\n\"\n                \"17. Température(Verre_type1) : 90 °C\\n\"\n                \"18. Statique/dynamique(Verre_type1) : Statique\\n\"\n                \"19. Plage granulo si poudre(Verre_type1) : 100-125 µm\\n\"\n                \"20. Surface spécifique géométrique si poudre(Verre_type1) : 19.9 cm²\\n\"\n                \"21. Surface spécifique BET si poudre(Verre_type1) : Non disponible\\n\"\n                \"22. Qualité polissage si monolithe(Verre_type1) : Non disponible\\n\"\n                \"23. Masse verre(Verre_type1) : 0.095 g\\n\"\n                \"24. S(verre)(Verre_type1) : 19.9 cm²\\n\"\n                \"25. V(solution)(Verre_type1) : 0.231 L\\n\"\n                \"26. Débit solution(Verre_type1) : Non disponible\\n\"\n                \"27. pH initial (T amb)(Verre_type1) : Non disponible\\n\"\n                \"28. pH initial (T essai)(Verre_type1) : 9\\n\"\n                \"29. Compo solution(Verre_type1) : Eau déionisée, pH ajusté à 9 avec LiOH 1M\\n\"\n                \"30. Durée expérience(Verre_type1) : 5.5 h\\n\"\n                \"31. pH final (T amb)(Verre_type1) : Non disponible\\n\"\n                \"32. pH final (T essai)(Verre_type1) : 9.0\\n\"\n                \"33. Normalisation vitesse (Spm ou SBET)(Verre_type1) : Surface géométrique\\n\"\n                \"34. V₀(Si)(Verre_type1) : Non disponible\\n\"\n                \"35. r²(Si)(Verre_type1) : 0.999\\n\"\n                \"36. Ordonnée origine(Verre_type1) : 0.2\\n\"\n                \"37. V₀(B)(Verre_type1) : Non disponible\\n\"\n                \"38. Ordonnée origine(Verre_type1) : Non disponible\\n\"\n                \"39. V₀(Na)(Verre_type1) : Non disponible\\n\"\n                \"40. r²(Na)(Verre_type1) : Non disponible\\n\"\n                \"41. Ordonnée origine(Verre_type1) : Non disponible\\n\"\n                \"42. V₀(ΔM)(Verre_type1) : Non disponible\\n\"\n                \"43. Congruence(Verre_type1) : Congruente (presque)\\n\"\n                \"44. Verre_type2 : NSAC19\\n\"\n                \"45. SiO₂(Verre_type2) : 55.3\\n\"\n                \"46. B₂O₃(Verre_type2) : 0\\n\"\n                \"47. Na₂O(Verre_type2) : 19.0\\n\"\n                \"48. Al₂O₃(Verre_type2) : 9.9\\n\"\n                \"49. Fines(Verre_type2) : 20-40 µm\\n\"\n                \"50. Densité(Verre_type2) : 2.591\\n\"\n                \"51. Homogénéité(Verre_type2) : Homogène\\n\"\n                \"52. % B(IV)(Verre_type2) : Non disponible\\n\"\n                \"53. Irradié(Verre_type2) : N\\n\"\n                \"54. Caractéristiques si irradié(Verre_type2) : Non disponible\\n\"\n                \"55. Température(Verre_type2) : 90 °C\\n\"\n                \"56. Statique/dynamique(Verre_type2) : Statique\\n\"\n                \"57. Plage granulo si poudre(Verre_type2) : 20-40 µm\\n\"\n                \"58. Surface spécifique géométrique si poudre(Verre_type2) : 132 cm²\\n\"\n                \"59. Surface spécifique BET si poudre(Verre_type2) : Non disponible\\n\"\n                \"60. Qualité polissage si monolithe(Verre_type2) : Non disponible\\n\"\n                \"61. Masse verre(Verre_type2) : 0.165 g\\n\"\n                \"62. S(verre)(Verre_type2) : 132 cm²\\n\"\n                \"63. V(solution)(Verre_type2) : 0.490 L\\n\"\n                \"64. Débit solution(Verre_type2) : Non disponible\\n\"\n                \"65. pH initial (T amb)(Verre_type2) : Non disponible\\n\"\n                \"66. pH initial (T essai)(Verre_type2) : 9\\n\"\n                \"67. Compo solution(Verre_type2) : Eau déionisée, pH ajusté à 9 avec LiOH 1M\\n\"\n                \"68. Durée expérience(Verre_type2) : 4.1 h\\n\"\n                \"69. pH final (T amb)(Verre_type2) : Non disponible\\n\"\n                \"70. pH final (T essai)(Verre_type2) : 8.8\\n\"\n                \"71. Normalisation vitesse (Spm ou SBET)(Verre_type2) : Surface géométrique\\n\"\n                \"72. V₀(Si)(Verre_type2) : Non disponible\\n\"\n                \"73. r²(Si)(Verre_type2) : 0.998\\n\"\n                \"74. Ordonnée origine(Verre_type2) : 0.05\\n\"\n                \"75. V₀(B)(Verre_type2) : Non disponible\\n\"\n                \"76. Ordonnée origine(Verre_type2) : Non disponible\\n\"\n                \"77. V₀(Na)(Verre_type2) : Non disponible\\n\"\n                \"78. r²(Na)(Verre_type2) : Non disponible\\n\"\n                \"79. Ordonnée origine(Verre_type2) : Non disponible\\n\"\n                \"80. V₀(ΔM)(Verre_type2) : Non disponible\\n\"\n                \"81. Congruence(Verre_type2) : Incongruente\\n\"\n                \"82. Verre_type3 : NSA\\n\"\n                \"83. SiO₂(Verre_type3) : 75.0\\n\"\n                \"84. B₂O₃(Verre_type3) : 0\\n\"\n                \"85. Na₂O(Verre_type3) : 12.5\\n\"\n                \"86. Al₂O₃(Verre_type3) : 12.5\\n\"\n                \"87. Fines(Verre_type3) : 50-100 µm\\n\"\n                \"88. Densité(Verre_type3) : 2.340\\n\"\n                \"89. Homogénéité(Verre_type3) : Homogène\\n\"\n                \"90. % B(IV)(Verre_type3) : Non disponible\\n\"\n                \"91. Irradié(Verre_type3) : N\\n\"\n                \"92. Caractéristiques si irradié(Verre_type3) : Non disponible\\n\"\n                \"93. Température(Verre_type3) : 90 °C\\n\"\n                \"94. Statique/dynamique(Verre_type3) : Statique\\n\"\n                \"95. Plage granulo si poudre(Verre_type3) : 50-100 µm\\n\"\n                \"96. Surface spécifique géométrique si poudre(Verre_type3) : 88.3 cm²\\n\"\n                \"97. Surface spécifique BET si poudre(Verre_type3) : Non disponible\\n\"\n                \"98. Qualité polissage si monolithe(Verre_type3) : Non disponible\\n\"\n                \"99. Masse verre(Verre_type3) : 0.276 g\\n\"\n                \"100. S(verre)(Verre_type3) : 88.3 cm²\\n\"\n                \"101. V(solution)(Verre_type3) : 0.482 L\\n\"\n                \"102. Débit solution(Verre_type3) : Non disponible\\n\"\n                \"103. pH initial (T amb)(Verre_type3) : Non disponible\\n\"\n                \"104. pH initial (T essai)(Verre_type3) : 9\\n\"\n                \"105. Compo solution(Verre_type3) : Eau déionisée, pH ajusté à 9 avec LiOH 1M\\n\"\n                \"106. Durée expérience(Verre_type3) : 121 h\\n\"\n                \"107. pH final (T amb)(Verre_type3) : Non disponible\\n\"\n                \"108. pH final (T essai)(Verre_type3) : 8.9\\n\"\n                \"109. Normalisation vitesse (Spm ou SBET)(Verre_type3) : Surface géométrique\\n\"\n                \"110. V₀(Si)(Verre_type3) : Non disponible\\n\"\n                \"111. r²(Si)(Verre_type3) : 0.995\\n\"\n                \"112. Ordonnée origine(Verre_type3) : 0.02\\n\"\n                \"113. V₀(B)(Verre_type3) : Non disponible\\n\"\n                \"114. Ordonnée origine(Verre_type3) : Non disponible\\n\"\n                \"115. V₀(Na)(Verre_type3) : Non disponible\\n\"\n                \"116. r²(Na)(Verre_type3) : Non disponible\\n\"\n                \"117. Ordonnée origine(Verre_type3) : Non disponible\\n\"\n                \"118. V₀(ΔM)(Verre_type3) : Non disponible\\n\"\n                \"119. Congruence(Verre_type3) : Congruente (presque)\\n\"\n                \"...\"\n                \"613. Congruence(Verre_type3) : Congruente (presque)\\n\"\n            ),\n            tool_mode=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Réponse\", name=\"sortie\", method=\"construire_sortie\"),\n    ]\n\n    def construire_sortie(self) -> Data:\n        texte_extrait = self.texte_extrait\n        print(f\"Texte Extrait: {texte_extrait}\")\n\n        try:\n            # Nettoyer et analyser le texte\n            lignes = [ligne.strip() for ligne in texte_extrait.split(\"\\n\") if ligne.strip()]\n\n            # Extraction des données générales\n            type_doc = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(\"1. Type du document :\")), None)\n            titre = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(\"2. Titre du document :\")), None)\n            reference = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(\"3. Référence :\")), None)\n            premier_auteur = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(\"4. Premier Auteur :\")), None)\n            nombre_types_verres = int(next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(\"5. Nombre de types de verres :\")), None))\n\n            # Initialisation des données pour chaque type de verre\n            donnees_verres = []\n\n            for i in range(nombre_types_verres):\n                verre_data = {}\n                verre_type_key = f\"Verre_type{i+1}\"\n                verre_data[\"type\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{6 + i * 38}. {verre_type_key} :\")), None)\n\n                # Extraction des caractéristiques pour chaque type de verre\n                verre_data[\"sio2\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{7 + i * 38}. SiO₂({verre_type_key}) :\")), None)\n                verre_data[\"b2o3\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{8 + i * 38}. B₂O₃({verre_type_key}) :\")), None)\n                verre_data[\"na2o\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{9 + i * 38}. Na₂O({verre_type_key}) :\")), None)\n                verre_data[\"al2o3\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{10 + i * 38}. Al₂O₃({verre_type_key}) :\")), None)\n                verre_data[\"fines\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{11 + i * 38}. Fines({verre_type_key}) :\")), None)\n                verre_data[\"densite\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{12 + i * 38}. Densité({verre_type_key}) :\")), None)\n                verre_data[\"homogeneite\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{13 + i * 38}. Homogénéité({verre_type_key}) :\")), None)\n                verre_data[\"b_iv_pourcent\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{14 + i * 38}. % B(IV)({verre_type_key}) :\")), None)\n                verre_data[\"irradie\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{15 + i * 38}. Irradié({verre_type_key}) :\")), None)\n                verre_data[\"caracteristiques_irradie\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{16 + i * 38}. Caractéristiques si irradié({verre_type_key}) :\")), None)\n                verre_data[\"temperature\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{17 + i * 38}. Température({verre_type_key}) :\")), None)\n                verre_data[\"statique_dynamique\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{18 + i * 38}. Statique/dynamique({verre_type_key}) :\")), None)\n                verre_data[\"plage_granulo\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{19 + i * 38}. Plage granulo si poudre({verre_type_key}) :\")), None)\n                verre_data[\"surface_specifique_geometrique\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{20 + i * 38}. Surface spécifique géométrique si poudre({verre_type_key}) :\")), None)\n                verre_data[\"surface_specifique_bet\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{21 + i * 38}. Surface spécifique BET si poudre({verre_type_key}) :\")), None)\n                verre_data[\"qualite_polissage\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{22 + i * 38}. Qualité polissage si monolithe({verre_type_key}) :\")), None)\n                verre_data[\"masse_verre\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{23 + i * 38}. Masse verre({verre_type_key}) :\")), None)\n                verre_data[\"s_verre\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{24 + i * 38}. S(verre)({verre_type_key}) :\")), None)\n                verre_data[\"v_solution\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{25 + i * 38}. V(solution)({verre_type_key}) :\")), None)\n                verre_data[\"debit_solution\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{26 + i * 38}. Débit solution({verre_type_key}) :\")), None)\n                verre_data[\"ph_initial\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{27 + i * 38}. pH initial (T amb)({verre_type_key}) :\")), None)\n                verre_data[\"ph_initial_test\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{28 + i * 38}. pH initial (T essai)({verre_type_key}) :\")), None)\n                verre_data[\"composition_solution\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{29 + i * 38}. Compo solution({verre_type_key}) :\")), None)\n                verre_data[\"duree_experience\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{30 + i * 38}. Durée expérience({verre_type_key}) :\")), None)\n                verre_data[\"ph_final_amb\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{31 + i * 38}. pH final (T amb)({verre_type_key}) :\")), None)\n                verre_data[\"ph_final_test\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{32 + i * 38}. pH final (T essai)({verre_type_key}) :\")), None)\n                verre_data[\"normalisation_vitesse\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{33 + i * 38}. Normalisation vitesse (Spm ou SBET)({verre_type_key}) :\")), None)\n                verre_data[\"v0_si\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{34 + i * 38}. V₀(Si)({verre_type_key}) :\")), None)\n                verre_data[\"r_carre_si\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{35 + i * 38}. r²(Si)({verre_type_key}) :\")), None)\n                verre_data[\"ordonnee_origine_si\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{38 + i * 38}. Ordonnée origine({verre_type_key}) :\")), None)\n                verre_data[\"v0_b\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{37 + i * 38}. V₀(B)({verre_type_key}) :\")), None)\n                verre_data[\"ordonnee_origine_b\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{38 + i * 38}. Ordonnée origine({verre_type_key}) :\")), None)\n                verre_data[\"v0_na\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{39 + i * 38}. V₀(Na)({verre_type_key}) :\")), None)\n                verre_data[\"r_carre_na\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{40 + i * 38}. r²(Na)({verre_type_key}) :\")), None)\n                verre_data[\"ordonnee_origine_na\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{41 + i * 38}. Ordonnée origine({verre_type_key}) :\")), None)\n                verre_data[\"v0_dm\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{42 + i * 38}. V₀(ΔM)({verre_type_key}) :\")), None)\n                verre_data[\"congruence\"] = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{43 + i * 38}. Congruence({verre_type_key}) :\")), None)\n\n                donnees_verres.append(verre_data)\n\n            # Préparer les données\n            url = 'http://127.0.0.1:5001/add_glass_data'\n            donnees = {\n                \"type\": type_doc,\n                \"titre\": titre,\n                \"reference\": reference,\n                \"premier_auteur\": premier_auteur,\n                \"nombre_types_verres\": nombre_types_verres,\n                \"verres\": donnees_verres\n            }\n            print(f\"Envoi des données: {donnees}\")\n\n            reponse = requests.post(url, json=donnees)\n\n            if reponse.status_code == 200:\n                return Data(value=\"Données du verre ajoutées avec succès!\")\n            else:\n                return Data(value=f\"Erreur lors de l'ajout des données du verre. Code d'état: {reponse.status_code} - {reponse.text}\")\n\n        except Exception as e:\n            return Data(value=f\"Exception survenue: {str(e)}\")\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "texte_extrait": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "texte_extrait",
                "value": "",
                "display_name": "Texte Extrait",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Texte extrait contenant la référence du document et les informations sur la composition du verre.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Envoyer la composition détaillée du verre et les informations de référence du document au serveur Flask.",
            "icon": "table",
            "base_classes": [
              "Data"
            ],
            "display_name": "DOCLING_LANGFLOW_FLASK_ONELINE",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "sortie",
                "display_name": "Réponse",
                "method": "construire_sortie",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "texte_extrait"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "CustomComponent",
          "id": "CustomComponent-OsOFR"
        },
        "selected": true,
        "width": 320,
        "height": 274,
        "positionAbsolute": {
          "x": 3217.1431542364103,
          "y": 1795.363494913002
        },
        "dragging": false
      },
      {
        "id": "File-yvKBt",
        "type": "genericNode",
        "position": {
          "x": 74.5160944434167,
          "y": 1624.5910127856118
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "path": {
                "trace_as_metadata": true,
                "file_path": "a7e4b6a1-d444-487c-bec7-a954e6d42725/2025-02-13_12-57-12_D2-plain.md",
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "zip"
                ],
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "path",
                "value": "",
                "display_name": "Path",
                "advanced": false,
                "dynamic": false,
                "info": "Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx, zip",
                "title_case": false,
                "type": "file",
                "_input_type": "FileInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from pathlib import Path\nfrom tempfile import NamedTemporaryFile\nfrom zipfile import ZipFile, is_zipfile\n\nfrom langflow.base.data.utils import TEXT_FILE_TYPES, parallel_load_data, parse_text_file_to_data\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, FileInput, IntInput, Output\nfrom langflow.schema import Data\n\n\nclass FileComponent(Component):\n    \"\"\"Handles loading of individual or zipped text files.\n\n    Processes multiple valid files within a zip archive if provided.\n\n    Attributes:\n        display_name: Display name of the component.\n        description: Brief component description.\n        icon: Icon to represent the component.\n        name: Identifier for the component.\n        inputs: Inputs required by the component.\n        outputs: Output of the component after processing files.\n    \"\"\"\n\n    display_name = \"File\"\n    description = \"Load a file to be used in your project.\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    inputs = [\n        FileInput(\n            name=\"path\",\n            display_name=\"Path\",\n            file_types=[*TEXT_FILE_TYPES, \"zip\"],\n            info=f\"Supported file types: {', '.join([*TEXT_FILE_TYPES, 'zip'])}\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n        BoolInput(\n            name=\"use_multithreading\",\n            display_name=\"Use Multithreading\",\n            advanced=True,\n            info=\"If true, parallel processing will be enabled for zip files.\",\n        ),\n        IntInput(\n            name=\"concurrency_multithreading\",\n            display_name=\"Multithreading Concurrency\",\n            advanced=True,\n            info=\"The maximum number of workers to use, if concurrency is enabled\",\n            value=4,\n        ),\n    ]\n\n    outputs = [Output(display_name=\"Data\", name=\"data\", method=\"load_file\")]\n\n    def load_file(self) -> Data:\n        \"\"\"Load and parse file(s) from a zip archive.\n\n        Raises:\n            ValueError: If no file is uploaded or file path is invalid.\n\n        Returns:\n            Data: Parsed data from file(s).\n        \"\"\"\n        # Check if the file path is provided\n        if not self.path:\n            self.log(\"File path is missing.\")\n            msg = \"Please upload a file for processing.\"\n\n            raise ValueError(msg)\n\n        resolved_path = Path(self.resolve_path(self.path))\n        try:\n            # Check if the file is a zip archive\n            if is_zipfile(resolved_path):\n                self.log(f\"Processing zip file: {resolved_path.name}.\")\n\n                return self._process_zip_file(\n                    resolved_path,\n                    silent_errors=self.silent_errors,\n                    parallel=self.use_multithreading,\n                )\n\n            self.log(f\"Processing single file: {resolved_path.name}.\")\n\n            return self._process_single_file(resolved_path, silent_errors=self.silent_errors)\n        except FileNotFoundError:\n            self.log(f\"File not found: {resolved_path.name}.\")\n\n            raise\n\n    def _process_zip_file(self, zip_path: Path, *, silent_errors: bool = False, parallel: bool = False) -> Data:\n        \"\"\"Process text files within a zip archive.\n\n        Args:\n            zip_path: Path to the zip file.\n            silent_errors: Suppresses errors if True.\n            parallel: Enables parallel processing if True.\n\n        Returns:\n            list[Data]: Combined data from all valid files.\n\n        Raises:\n            ValueError: If no valid files found in the archive.\n        \"\"\"\n        data: list[Data] = []\n        with ZipFile(zip_path, \"r\") as zip_file:\n            # Filter file names based on extensions in TEXT_FILE_TYPES and ignore hidden files\n            valid_files = [\n                name\n                for name in zip_file.namelist()\n                if (\n                    any(name.endswith(ext) for ext in TEXT_FILE_TYPES)\n                    and not name.startswith(\"__MACOSX\")\n                    and not name.startswith(\".\")\n                )\n            ]\n\n            # Raise an error if no valid files found\n            if not valid_files:\n                self.log(\"No valid files in the zip archive.\")\n\n                # Return empty data if silent_errors is True\n                if silent_errors:\n                    return data  # type: ignore[return-value]\n\n                # Raise an error if no valid files found\n                msg = \"No valid files in the zip archive.\"\n                raise ValueError(msg)\n\n            # Define a function to process each file\n            def process_file(file_name, silent_errors=silent_errors):\n                with NamedTemporaryFile(delete=False) as temp_file:\n                    temp_path = Path(temp_file.name).with_name(file_name)\n                    with zip_file.open(file_name) as file_content:\n                        temp_path.write_bytes(file_content.read())\n                try:\n                    return self._process_single_file(temp_path, silent_errors=silent_errors)\n                finally:\n                    temp_path.unlink()\n\n            # Process files in parallel if specified\n            if parallel:\n                self.log(\n                    f\"Initializing parallel Thread Pool Executor with max workers: \"\n                    f\"{self.concurrency_multithreading}.\"\n                )\n\n                # Process files in parallel\n                initial_data = parallel_load_data(\n                    valid_files,\n                    silent_errors=silent_errors,\n                    load_function=process_file,\n                    max_concurrency=self.concurrency_multithreading,\n                )\n\n                # Filter out empty data\n                data = list(filter(None, initial_data))\n            else:\n                # Sequential processing\n                data = [process_file(file_name) for file_name in valid_files]\n\n        self.log(f\"Successfully processed zip file: {zip_path.name}.\")\n\n        return data  # type: ignore[return-value]\n\n    def _process_single_file(self, file_path: Path, *, silent_errors: bool = False) -> Data:\n        \"\"\"Process a single file.\n\n        Args:\n            file_path: Path to the file.\n            silent_errors: Suppresses errors if True.\n\n        Returns:\n            Data: Parsed data from the file.\n\n        Raises:\n            ValueError: For unsupported file formats.\n        \"\"\"\n        # Check if the file type is supported\n        if not any(file_path.suffix == ext for ext in [\".\" + f for f in TEXT_FILE_TYPES]):\n            self.log(f\"Unsupported file type: {file_path.suffix}\")\n\n            # Return empty data if silent_errors is True\n            if silent_errors:\n                return Data()\n\n            msg = f\"Unsupported file type: {file_path.suffix}\"\n            raise ValueError(msg)\n\n        try:\n            # Parse the text file as appropriate\n            data = parse_text_file_to_data(str(file_path), silent_errors=silent_errors)  # type: ignore[assignment]\n            if not data:\n                data = Data()\n\n            self.log(f\"Successfully processed file: {file_path.name}.\")\n        except Exception as e:\n            self.log(f\"Error processing file {file_path.name}: {e}\")\n\n            # Return empty data if silent_errors is True\n            if not silent_errors:\n                raise\n\n            data = Data()\n\n        return data\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "concurrency_multithreading": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "concurrency_multithreading",
                "value": 4,
                "display_name": "Multithreading Concurrency",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of workers to use, if concurrency is enabled",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "silent_errors": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "silent_errors",
                "value": false,
                "display_name": "Silent Errors",
                "advanced": true,
                "dynamic": false,
                "info": "If true, errors will not raise an exception.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "use_multithreading": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "use_multithreading",
                "value": false,
                "display_name": "Use Multithreading",
                "advanced": true,
                "dynamic": false,
                "info": "If true, parallel processing will be enabled for zip files.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Load a file to be used in your project.",
            "icon": "file-text",
            "base_classes": [
              "Data"
            ],
            "display_name": "File",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "data",
                "display_name": "Data",
                "method": "load_file",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "path",
              "silent_errors",
              "use_multithreading",
              "concurrency_multithreading"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "File",
          "id": "File-yvKBt"
        },
        "selected": false,
        "width": 320,
        "height": 232,
        "positionAbsolute": {
          "x": 74.5160944434167,
          "y": 1624.5910127856118
        },
        "dragging": false
      },
      {
        "id": "GoogleGenerativeAIModel-Efdt7",
        "type": "genericNode",
        "position": {
          "x": 1592.272697577117,
          "y": 892.1892301896338
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "output_parser": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "output_parser",
                "value": "",
                "display_name": "Output Parser",
                "advanced": true,
                "input_types": [
                  "OutputParser"
                ],
                "dynamic": false,
                "info": "The parser to use to parse the output of the model",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n        ),\n        SecretStrInput(\n            name=\"google_api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.google_api_key\n        model = self.model\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "google_api_key": {
                "load_from_db": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "google_api_key",
                "value": "GeminiApiKey",
                "display_name": "Google API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 10000000,
                "display_name": "Max Output Tokens",
                "advanced": false,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "model": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-1.0-pro",
                  "gemini-1.0-pro-vision",
                  "gemini-1.5-pro-001",
                  "gemini-1.5-pro-002",
                  "gemini-1.5-flash-001",
                  "gemini-1.5-flash-002",
                  "gemini-1.5-pro-exp-0827",
                  "gemini-1.5-flash-exp-0827",
                  "gemini-1.5-flash-8b-exp-0827",
                  "gemini-1.5-flash-8b-exp-0924",
                  "gemini-exp-1114"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model",
                "value": "gemini-1.5-flash",
                "display_name": "Model",
                "advanced": false,
                "dynamic": false,
                "info": "The name of the model to use.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "n": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": false,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.2,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "top_k": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Generate text using Google Generative AI.",
            "icon": "GoogleGenerativeAI",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "Google Generative AI",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": []
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": []
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model",
              "google_api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "output_parser"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "GoogleGenerativeAIModel",
          "id": "GoogleGenerativeAIModel-Efdt7"
        },
        "selected": false,
        "width": 320,
        "height": 760,
        "dragging": false,
        "positionAbsolute": {
          "x": 1592.272697577117,
          "y": 892.1892301896338
        }
      },
      {
        "id": "TextInput-ITO2y",
        "type": "genericNode",
        "position": {
          "x": 1037.3570432684696,
          "y": 1697.0527526753974
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "Faire ce qui est demandé dans le prompt",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text to be passed as input.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "Get text inputs from the Playground.",
            "icon": "type",
            "base_classes": [
              "Message"
            ],
            "display_name": "Text Input",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "TextInput",
          "id": "TextInput-ITO2y"
        },
        "selected": false,
        "width": 320,
        "height": 234,
        "positionAbsolute": {
          "x": 1037.3570432684696,
          "y": 1697.0527526753974
        },
        "dragging": false
      },
      {
        "id": "Prompt-LSaLj",
        "type": "genericNode",
        "position": {
          "x": 1038.041161653572,
          "y": 2084.857604494462
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-LSaLj",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "Veuillez lire le document ci-dessous et extraire les informations demandées. Formatez les réponses clairement pour chaque type de verre et essayez de faire de votre mieux pour extraire toutes les informations de la meilleure façon possible, sans demander à l'utilisateur de continuer à remplir les valeurs. Utilisez le format suivant pour chaque type de verre :\n\n---\n\n**Document :**\n{Document}\n\n---\n\n**Informations demandées :**\n\n**Pour les types de verres 9 à 16 :**\n\n310. Verre_type9 : [Description ou identifiant du type de verre]\n311. SiO₂(Verre_type9) : [Valeur numérique]\n312. B₂O₃(Verre_type9) : [Valeur numérique]\n313. Na₂O(Verre_type9) : [Valeur numérique]\n314. Al₂O₃(Verre_type9) : [Valeur numérique]\n315. Fines(Verre_type9) : [Valeur numérique]\n316. Densité(Verre_type9) : [Valeur numérique]\n317. Homogénéité(Verre_type9) : [Commentaire]\n318. % B(IV)(Verre_type9) : [Valeur numérique]\n319. Irradié(Verre_type9) : [O/N]\n320. Caractéristiques si irradié(Verre_type9) : [Commentaire]\n321. Température(Verre_type9) : [Valeur numérique]\n322. Statique/dynamique(Verre_type9) : [Commentaire]\n323. Plage granulo si poudre(Verre_type9) : [Valeur numérique]\n324. Surface spécifique géométrique si poudre(Verre_type9) : [Valeur numérique]\n325. Surface spécifique BET si poudre(Verre_type9) : [Valeur numérique]\n326. Qualité polissage si monolithe(Verre_type9) : [Commentaire]\n327. Masse verre(Verre_type9) : [Valeur numérique]\n328. S(verre)(Verre_type9) : [Valeur numérique]\n329. V(solution)(Verre_type9) : [Valeur numérique]\n330. Débit solution(Verre_type9) : [Valeur numérique]\n331. pH initial (T amb)(Verre_type9) : [Valeur numérique]\n332. pH initial (T essai)(Verre_type9) : [Valeur numérique]\n333. Compo solution(Verre_type9) : [Commentaire]\n334. Durée expérience(Verre_type9) : [Valeur numérique]\n335. pH final (T amb)(Verre_type9) : [Valeur numérique]\n336. pH final (T essai)(Verre_type9) : [Valeur numérique]\n337. Normalisation vitesse (Spm ou SBET)(Verre_type9) : [Valeur numérique]\n338. V₀(Si)(Verre_type9) : [Valeur numérique]\n339. r²(Si)(Verre_type9) : [Valeur numérique]\n340. Ordonnée origine(Verre_type9) : [Valeur numérique]\n341. V₀(B)(Verre_type9) : [Valeur numérique]\n342. Ordonnée origine(Verre_type9) : [Valeur numérique]\n343. V₀(Na)(Verre_type9) : [Valeur numérique]\n344. r²(Na)(Verre_type9) : [Valeur numérique]\n345. Ordonnée origine(Verre_type9) : [Valeur numérique]\n346. V₀(ΔM)(Verre_type9) : [Valeur numérique]\n347. Congruence(Verre_type9) : [Commentaire]\n\n**Répétez les étapes ci-dessus pour Verre_type10, Verre_type11, Verre_type12, Verre_type13, Verre_type14, Verre_type15 et Verre_type16 en incrémentant les numéros.**\n\n---\n\n**Format attendu :**\n\n310. Verre_type9 : [Type9]\n311. SiO₂(Verre_type9) : [Valeur]\n312. B₂O₃(Verre_type9) : [Valeur]\n...\n347. Congruence(Verre_type9) : [Commentaire]\n348. Verre_type10 : [Type10]\n349. SiO₂(Verre_type10) : [Valeur]\n350. B₂O₃(Verre_type10) : [Valeur]\n...\n382. Congruence(Verre_type10) : [Commentaire]\n383. Verre_type11 : [Type11]\n384. SiO₂(Verre_type11) : [Valeur]\n385. B₂O₃(Verre_type11) : [Valeur]\n...\n416. Congruence(Verre_type11) : [Commentaire]\n...\n613. Congruence(Verre_type16) : [Commentaire]\n\n---\n\nIl faut absolument donner toutes les valeurs des 8 types de verres de 310 à 613, et il ne faut surtout pas ajouter de commentaire à la fin. Si une des informations est indisponible dans le document, indiquez \"Non disponible\"."
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt1Line",
            "documentation": "",
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "Prompt"
        },
        "selected": false,
        "width": 320,
        "height": 346,
        "positionAbsolute": {
          "x": 1038.041161653572,
          "y": 2084.857604494462
        },
        "dragging": false
      },
      {
        "id": "GoogleGenerativeAIModel-5z5kt",
        "type": "genericNode",
        "position": {
          "x": 1605.0327684959416,
          "y": 1756.2500832191981
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "output_parser": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "output_parser",
                "value": "",
                "display_name": "Output Parser",
                "advanced": true,
                "input_types": [
                  "OutputParser"
                ],
                "dynamic": false,
                "info": "The parser to use to parse the output of the model",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n        ),\n        SecretStrInput(\n            name=\"google_api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.google_api_key\n        model = self.model\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "google_api_key": {
                "load_from_db": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "google_api_key",
                "value": "GeminiApiKey",
                "display_name": "Google API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000000,
                "display_name": "Max Output Tokens",
                "advanced": false,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "model": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-1.0-pro",
                  "gemini-1.0-pro-vision",
                  "gemini-1.5-pro-001",
                  "gemini-1.5-pro-002",
                  "gemini-1.5-flash-001",
                  "gemini-1.5-flash-002",
                  "gemini-1.5-pro-exp-0827",
                  "gemini-1.5-flash-exp-0827",
                  "gemini-1.5-flash-8b-exp-0827",
                  "gemini-1.5-flash-8b-exp-0924",
                  "gemini-exp-1114"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model",
                "value": "gemini-1.5-flash",
                "display_name": "Model",
                "advanced": false,
                "dynamic": false,
                "info": "The name of the model to use.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "n": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": false,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.2,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "top_k": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Generate text using Google Generative AI.",
            "icon": "GoogleGenerativeAI",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "Google Generative AI",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": []
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": []
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model",
              "google_api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "output_parser"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "GoogleGenerativeAIModel",
          "id": "GoogleGenerativeAIModel-5z5kt"
        },
        "selected": false,
        "width": 320,
        "height": 760,
        "positionAbsolute": {
          "x": 1605.0327684959416,
          "y": 1756.2500832191981
        },
        "dragging": false
      },
      {
        "id": "CombineText-AAfQ7",
        "type": "genericNode",
        "position": {
          "x": 2138.007242200039,
          "y": 1586.986830038834
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " ",
                "display_name": "Delimiter",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "CombineText",
          "id": "CombineText-AAfQ7"
        },
        "selected": false,
        "width": 320,
        "height": 427,
        "positionAbsolute": {
          "x": 2138.007242200039,
          "y": 1586.986830038834
        },
        "dragging": false
      },
      {
        "id": "GoogleGenerativeAIModel-MUVu4",
        "type": "genericNode",
        "position": {
          "x": 1603.6569472467354,
          "y": 2645.320928111027
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "output_parser": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "output_parser",
                "value": "",
                "display_name": "Output Parser",
                "advanced": true,
                "input_types": [
                  "OutputParser"
                ],
                "dynamic": false,
                "info": "The parser to use to parse the output of the model",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n        ),\n        SecretStrInput(\n            name=\"google_api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.google_api_key\n        model = self.model\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "google_api_key": {
                "load_from_db": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "google_api_key",
                "value": "GeminiApiKey",
                "display_name": "Google API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000000,
                "display_name": "Max Output Tokens",
                "advanced": false,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "model": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-1.0-pro",
                  "gemini-1.0-pro-vision",
                  "gemini-1.5-pro-001",
                  "gemini-1.5-pro-002",
                  "gemini-1.5-flash-001",
                  "gemini-1.5-flash-002",
                  "gemini-1.5-pro-exp-0827",
                  "gemini-1.5-flash-exp-0827",
                  "gemini-1.5-flash-8b-exp-0827",
                  "gemini-1.5-flash-8b-exp-0924",
                  "gemini-exp-1114"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model",
                "value": "gemini-1.5-flash",
                "display_name": "Model",
                "advanced": false,
                "dynamic": false,
                "info": "The name of the model to use.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "n": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": false,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.2,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "top_k": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Generate text using Google Generative AI.",
            "icon": "GoogleGenerativeAI",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "Google Generative AI",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": []
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": []
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model",
              "google_api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "output_parser"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "GoogleGenerativeAIModel",
          "id": "GoogleGenerativeAIModel-MUVu4"
        },
        "selected": false,
        "width": 320,
        "height": 760,
        "positionAbsolute": {
          "x": 1603.6569472467354,
          "y": 2645.320928111027
        },
        "dragging": false
      },
      {
        "id": "CombineText-wX6Af",
        "type": "genericNode",
        "position": {
          "x": 2721.6865223745804,
          "y": 1610.5710233837658
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " ",
                "display_name": "Delimiter",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "CombineText",
          "id": "CombineText-wX6Af"
        },
        "selected": false,
        "width": 320,
        "height": 427,
        "positionAbsolute": {
          "x": 2721.6865223745804,
          "y": 1610.5710233837658
        },
        "dragging": false
      },
      {
        "id": "Prompt-WdUv1",
        "type": "genericNode",
        "position": {
          "x": 1043.6011638028046,
          "y": 2707.3642825594634
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-WdUv1",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "Veuillez lire le document ci-dessous et extraire les informations demandées. Formatez les réponses clairement pour chaque type de verre et essayez de faire de votre mieux pour extraire toutes les informations de la meilleure façon possible, sans demander à l'utilisateur de continuer à remplir les valeurs. Utilisez le format suivant pour chaque type de verre :\n\n---\n\n**Document :**\n{Document}\n\n---\n\n**Informations demandées :**\n\n**Si et seulement si le nombre de types de verres est supérieur à 16, extrayez les informations pour les types de verres 17 à 25 :**\n\n614. Verre_type17 : [Description ou identifiant du type de verre]\n615. SiO₂(Verre_type17) : [Valeur numérique]\n616. B₂O₃(Verre_type17) : [Valeur numérique]\n617. Na₂O(Verre_type17) : [Valeur numérique]\n618. Al₂O₃(Verre_type17) : [Valeur numérique]\n619. Fines(Verre_type17) : [Valeur numérique]\n620. Densité(Verre_type17) : [Valeur numérique]\n621. Homogénéité(Verre_type17) : [Commentaire]\n622. % B(IV)(Verre_type17) : [Valeur numérique]\n623. Irradié(Verre_type17) : [O/N]\n624. Caractéristiques si irradié(Verre_type17) : [Commentaire]\n625. Température(Verre_type17) : [Valeur numérique]\n626. Statique/dynamique(Verre_type17) : [Commentaire]\n627. Plage granulo si poudre(Verre_type17) : [Valeur numérique]\n628. Surface spécifique géométrique si poudre(Verre_type17) : [Valeur numérique]\n629. Surface spécifique BET si poudre(Verre_type17) : [Valeur numérique]\n630. Qualité polissage si monolithe(Verre_type17) : [Commentaire]\n631. Masse verre(Verre_type17) : [Valeur numérique]\n632. S(verre)(Verre_type17) : [Valeur numérique]\n633. V(solution)(Verre_type17) : [Valeur numérique]\n634. Débit solution(Verre_type17) : [Valeur numérique]\n635. pH initial (T amb)(Verre_type17) : [Valeur numérique]\n636. pH initial (T essai)(Verre_type17) : [Valeur numérique]\n637. Compo solution(Verre_type17) : [Commentaire]\n638. Durée expérience(Verre_type17) : [Valeur numérique]\n639. pH final (T amb)(Verre_type17) : [Valeur numérique]\n640. pH final (T essai)(Verre_type17) : [Valeur numérique]\n641. Normalisation vitesse (Spm ou SBET)(Verre_type17) : [Valeur numérique]\n642. V₀(Si)(Verre_type17) : [Valeur numérique]\n643. r²(Si)(Verre_type17) : [Valeur numérique]\n644. Ordonnée origine(Verre_type17) : [Valeur numérique]\n645. V₀(B)(Verre_type17) : [Valeur numérique]\n646. Ordonnée origine(Verre_type17) : [Valeur numérique]\n647. V₀(Na)(Verre_type17) : [Valeur numérique]\n648. r²(Na)(Verre_type17) : [Valeur numérique]\n649. Ordonnée origine(Verre_type17) : [Valeur numérique]\n650. V₀(ΔM)(Verre_type17) : [Valeur numérique]\n651. Congruence(Verre_type17) : [Commentaire]\n\n**Répétez les étapes ci-dessus pour Verre_type18, Verre_type19, Verre_type20, Verre_type21, Verre_type22, Verre_type23, Verre_type24 et Verre_type25 en incrémentant les numéros.**\n\n---\n\n**Format attendu :**\n\n614. Verre_type17 : [Type17]\n615. SiO₂(Verre_type17) : [Valeur]\n616. B₂O₃(Verre_type17) : [Valeur]\n...\n651. Congruence(Verre_type17) : [Commentaire]\n652. Verre_type18 : [Type18]\n653. SiO₂(Verre_type18) : [Valeur]\n654. B₂O₃(Verre_type18) : [Valeur]\n...\n686. Congruence(Verre_type18) : [Commentaire]\n687. Verre_type19 : [Type19]\n688. SiO₂(Verre_type19) : [Valeur]\n689. B₂O₃(Verre_type19) : [Valeur]\n...\n721. Congruence(Verre_type19) : [Commentaire]\n...\n912. Congruence(Verre_type25) : [Commentaire]\n\n---\nEn cas où le nombre de verres ne dépasse pas 16 je veux que tu remplace toutes les valeurs par \"None\"\nIl faut absolument donner toutes les valeurs des 9 types de verres de 614 à 912, et il ne faut surtout pas ajouter de commentaire à la fin. Si une des informations est indisponible dans le document, indiquez \"Non disponible\"."
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt1Line",
            "documentation": "",
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "Prompt"
        },
        "selected": false,
        "width": 320,
        "height": 346,
        "positionAbsolute": {
          "x": 1043.6011638028046,
          "y": 2707.3642825594634
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "source": "ParseData-ggwft",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-ggwftœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-F8PHW",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-F8PHWœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-F8PHW",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-ggwft",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ParseData-ggwft{œdataTypeœ:œParseDataœ,œidœ:œParseData-ggwftœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-F8PHW{œfieldNameœ:œDocumentœ,œidœ:œPrompt-F8PHWœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "animated": false,
        "className": "ran"
      },
      {
        "source": "File-yvKBt",
        "sourceHandle": "{œdataTypeœ:œFileœ,œidœ:œFile-yvKBtœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}",
        "target": "ParseData-ggwft",
        "targetHandle": "{œfieldNameœ:œdataœ,œidœ:œParseData-ggwftœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "data",
            "id": "ParseData-ggwft",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "File",
            "id": "File-yvKBt",
            "name": "data",
            "output_types": [
              "Data"
            ]
          }
        },
        "id": "reactflow__edge-File-yvKBt{œdataTypeœ:œFileœ,œidœ:œFile-yvKBtœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-ParseData-ggwft{œfieldNameœ:œdataœ,œidœ:œParseData-ggwftœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "animated": false,
        "className": "not-running"
      },
      {
        "source": "Prompt-F8PHW",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-F8PHWœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-Efdt7",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-Efdt7œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-Efdt7",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-F8PHW",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Prompt-F8PHW{œdataTypeœ:œPromptœ,œidœ:œPrompt-F8PHWœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-Efdt7{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-Efdt7œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "animated": true,
        "className": "running"
      },
      {
        "source": "TextInput-ITO2y",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-ITO2yœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-Efdt7",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-Efdt7œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-Efdt7",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-ITO2y",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-TextInput-ITO2y{œdataTypeœ:œTextInputœ,œidœ:œTextInput-ITO2yœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-Efdt7{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-Efdt7œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "not-running",
        "animated": false
      },
      {
        "source": "ParseData-ggwft",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-ggwftœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-LSaLj",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-LSaLjœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-LSaLj",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-ggwft",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ParseData-ggwft{œdataTypeœ:œParseDataœ,œidœ:œParseData-ggwftœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-LSaLj{œfieldNameœ:œDocumentœ,œidœ:œPrompt-LSaLjœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "className": "ran",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-Efdt7",
        "sourceHandle": "{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-Efdt7œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-AAfQ7",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-AAfQ7œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-AAfQ7",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "GoogleGenerativeAIModel",
            "id": "GoogleGenerativeAIModel-Efdt7",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-GoogleGenerativeAIModel-Efdt7{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-Efdt7œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-AAfQ7{œfieldNameœ:œtext1œ,œidœ:œCombineText-AAfQ7œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "not-running",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-5z5kt",
        "sourceHandle": "{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-5z5ktœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-AAfQ7",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-AAfQ7œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-AAfQ7",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "GoogleGenerativeAIModel",
            "id": "GoogleGenerativeAIModel-5z5kt",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-GoogleGenerativeAIModel-5z5kt{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-5z5ktœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-AAfQ7{œfieldNameœ:œtext2œ,œidœ:œCombineText-AAfQ7œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "not-running",
        "animated": false
      },
      {
        "source": "Prompt-LSaLj",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-LSaLjœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-5z5kt",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-5z5ktœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-5z5kt",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-LSaLj",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Prompt-LSaLj{œdataTypeœ:œPromptœ,œidœ:œPrompt-LSaLjœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-5z5kt{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-5z5ktœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "running",
        "animated": true
      },
      {
        "source": "TextInput-ITO2y",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-ITO2yœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-5z5kt",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-5z5ktœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-5z5kt",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-ITO2y",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-TextInput-ITO2y{œdataTypeœ:œTextInputœ,œidœ:œTextInput-ITO2yœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-5z5kt{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-5z5ktœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "not-running",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-MUVu4",
        "sourceHandle": "{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-MUVu4œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-wX6Af",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-wX6Afœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-wX6Af",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "GoogleGenerativeAIModel",
            "id": "GoogleGenerativeAIModel-MUVu4",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-GoogleGenerativeAIModel-MUVu4{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-MUVu4œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-wX6Af{œfieldNameœ:œtext2œ,œidœ:œCombineText-wX6Afœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "animated": false,
        "className": "not-running"
      },
      {
        "source": "CombineText-AAfQ7",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-AAfQ7œ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-wX6Af",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-wX6Afœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-wX6Af",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-AAfQ7",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-CombineText-AAfQ7{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-AAfQ7œ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-CombineText-wX6Af{œfieldNameœ:œtext1œ,œidœ:œCombineText-wX6Afœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "animated": false,
        "className": "not-running"
      },
      {
        "source": "ParseData-ggwft",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-ggwftœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-WdUv1",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-WdUv1œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-WdUv1",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-ggwft",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ParseData-ggwft{œdataTypeœ:œParseDataœ,œidœ:œParseData-ggwftœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-WdUv1{œfieldNameœ:œDocumentœ,œidœ:œPrompt-WdUv1œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "animated": false,
        "className": "ran"
      },
      {
        "source": "Prompt-WdUv1",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-WdUv1œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-MUVu4",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-MUVu4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-MUVu4",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-WdUv1",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Prompt-WdUv1{œdataTypeœ:œPromptœ,œidœ:œPrompt-WdUv1œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-MUVu4{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-MUVu4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "animated": true,
        "className": "running"
      },
      {
        "source": "CombineText-wX6Af",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-wX6Afœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-OsOFR",
        "targetHandle": "{œfieldNameœ:œtexte_extraitœ,œidœ:œCustomComponent-OsOFRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "texte_extrait",
            "id": "CustomComponent-OsOFR",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-wX6Af",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-CombineText-wX6Af{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-wX6Afœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-OsOFR{œfieldNameœ:œtexte_extraitœ,œidœ:œCustomComponent-OsOFRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "animated": false,
        "className": "not-running"
      },
      {
        "source": "TextInput-ITO2y",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-ITO2yœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-MUVu4",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-MUVu4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-MUVu4",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-ITO2y",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-TextInput-ITO2y{œdataTypeœ:œTextInputœ,œidœ:œTextInput-ITO2yœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-MUVu4{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-MUVu4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "animated": false,
        "className": "not-running"
      }
    ],
    "viewport": {
      "x": 74.29744779759812,
      "y": -419.3091406343982,
      "zoom": 0.3966767212037015
    }
  },
  "description": null,
  "name": "Docling_Langflow_flask",
  "last_tested_version": "1.1.1",
  "endpoint_name": null,
  "is_component": false
}